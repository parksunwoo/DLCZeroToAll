{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi task learning _ Entity Taggging &  Intent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd \n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"trainset.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
    "validation_raw = pd.read_csv(\"test_hidden.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
    "#validation_raw = pd.read_csv(\"validation.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>area</td>\n",
       "      <td>EECCCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>자강의 면적은 얼마 정도되는지 알려줄래</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCCCCCCCCCCEEECCCCCCCCCCCC</td>\n",
       "      <td>WIKI PEDIA로 변재일 생년월일을 알고 싶어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>EEEEEEEEEEECCCCCCCCCCCCCCCCC</td>\n",
       "      <td>남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>length</td>\n",
       "      <td>EEEECCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>삼양터널의 총 길이 위키백과사전에서 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>EEEEEECCCCCCCCCCC</td>\n",
       "      <td>코니 윌리스의 태어난 곳은 뭐지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weight</td>\n",
       "      <td>CCCCCCCCCCCCEEEECCCCCCCCCCCCC</td>\n",
       "      <td>WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>definition</td>\n",
       "      <td>CCCCCCCCCCCCCEEECCCCCCCC</td>\n",
       "      <td>WIKIPEDIA백과로 라이프 찾아서 말해줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>height</td>\n",
       "      <td>EEEEEEEECCCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCEEEEEECCCCCCCCCCCCCCC</td>\n",
       "      <td>검색 HLKVAM 언제 출생했는지를 검색해라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>height</td>\n",
       "      <td>CCCCCCCCEEEEEECCCCCCCC</td>\n",
       "      <td>위키 피디아에 푸조 508 전고가 몇이야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>length</td>\n",
       "      <td>CCCEEEEECCCCCCC</td>\n",
       "      <td>검색 호몬혼 섬 길이를 찾아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>definition</td>\n",
       "      <td>EEEEECCCCCCCCCCCCC</td>\n",
       "      <td>영산중학교 좀 위키피디아사전 검색</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>age</td>\n",
       "      <td>CCCCCCEEEEEECCCCCCC</td>\n",
       "      <td>위키백과로 침보라조 산 나이 어떤지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>EEEEEEECCCCCCCC</td>\n",
       "      <td>마무드 아스라의 출생 찾아줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>CCCCCCEEEEEEECCCCCCCCC</td>\n",
       "      <td>위키 백과 조제 카리오카의 출생지를 찾아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCEEEEEECCCCCCCCC</td>\n",
       "      <td>검색 제이 개츠비 생년월일은 뭐지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>length</td>\n",
       "      <td>EEEECCCCCCCCCCCCCCCCC</td>\n",
       "      <td>증약터널의 길이가 얼마쯤인지 혹시 알아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>belong_to</td>\n",
       "      <td>EEEEEEEEEEEEEEEECCCCCCCCCCCCCC</td>\n",
       "      <td>리히텐슈타인의 한스 아담 2세 소속사는 어딘지 검색해봐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>height</td>\n",
       "      <td>CCCCCCCCCCCCEEEEECCCCCCCCC</td>\n",
       "      <td>WIKI사전백과 검색 벨록스여우의 높이는 얼만지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>age</td>\n",
       "      <td>EEEEEECCCCCCCCC</td>\n",
       "      <td>파블롭스키구의 나이를 찾아줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>width</td>\n",
       "      <td>EEEEEEECCCCCCCCCCCCCC</td>\n",
       "      <td>사카피솔라 섬의 너비는 WIKI에서 뭐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>EECCCCCCCCCCCCCCCCC</td>\n",
       "      <td>나미는 태어난 곳이 WIKI로 뭔지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>weight</td>\n",
       "      <td>CCCCCEEEEECCCCCCC</td>\n",
       "      <td>위키에서 피니스테르의 무게 찾기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>CCCEEEEEEECCCCCCCCCCC</td>\n",
       "      <td>검색 카를 야스퍼스 출신지역이 어디라고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>width</td>\n",
       "      <td>EEEEEEEEEEECCCCCCC</td>\n",
       "      <td>63식 병력수송장갑차의 폭 얼만지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>CCCCCEEECCCCCCCCCCCC</td>\n",
       "      <td>검색으로 강마에가 출생 장소를 찾아줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>EEEEEECCCCCCCCCCCCCC</td>\n",
       "      <td>쿠죠 히카리의 언제 출생했는지 탐색해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>length</td>\n",
       "      <td>EEECCCCCCCCCCC</td>\n",
       "      <td>사하라의 길이가 얼마쯤이지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>area</td>\n",
       "      <td>EEEECCCCCCCCC</td>\n",
       "      <td>송대산성의 면적은 얼만지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>area</td>\n",
       "      <td>CCCCCCCCCCEEEEEECCCCCCC</td>\n",
       "      <td>WIKI 피디아에 신자경선생묘의 넓이 뭔지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intent                          entity  \\\n",
       "0          area           EECCCCCCCCCCCCCCCCCCC   \n",
       "1    birth_date     CCCCCCCCCCCCEEECCCCCCCCCCCC   \n",
       "2           age    EEEEEEEEEEECCCCCCCCCCCCCCCCC   \n",
       "3        length          EEEECCCCCCCCCCCCCCCCCC   \n",
       "4   birth_place               EEEEEECCCCCCCCCCC   \n",
       "5        weight   CCCCCCCCCCCCEEEECCCCCCCCCCCCC   \n",
       "6    definition        CCCCCCCCCCCCCEEECCCCCCCC   \n",
       "7        height     EEEEEEEECCCCCCCCCCCCCCCCCCC   \n",
       "8    birth_date        CCCEEEEEECCCCCCCCCCCCCCC   \n",
       "9        height          CCCCCCCCEEEEEECCCCCCCC   \n",
       "10       length                 CCCEEEEECCCCCCC   \n",
       "11   definition              EEEEECCCCCCCCCCCCC   \n",
       "12          age             CCCCCCEEEEEECCCCCCC   \n",
       "13   birth_date                 EEEEEEECCCCCCCC   \n",
       "14  birth_place          CCCCCCEEEEEEECCCCCCCCC   \n",
       "15   birth_date              CCCEEEEEECCCCCCCCC   \n",
       "16       length           EEEECCCCCCCCCCCCCCCCC   \n",
       "17    belong_to  EEEEEEEEEEEEEEEECCCCCCCCCCCCCC   \n",
       "18       height      CCCCCCCCCCCCEEEEECCCCCCCCC   \n",
       "19          age                 EEEEEECCCCCCCCC   \n",
       "20        width           EEEEEEECCCCCCCCCCCCCC   \n",
       "21  birth_place             EECCCCCCCCCCCCCCCCC   \n",
       "22       weight               CCCCCEEEEECCCCCCC   \n",
       "23  birth_place           CCCEEEEEEECCCCCCCCCCC   \n",
       "24        width              EEEEEEEEEEECCCCCCC   \n",
       "25  birth_place            CCCCCEEECCCCCCCCCCCC   \n",
       "26   birth_date            EEEEEECCCCCCCCCCCCCC   \n",
       "27       length                  EEECCCCCCCCCCC   \n",
       "28         area                   EEEECCCCCCCCC   \n",
       "29         area         CCCCCCCCCCEEEEEECCCCCCC   \n",
       "\n",
       "                          sentence  \n",
       "0            자강의 면적은 얼마 정도되는지 알려줄래  \n",
       "1      WIKI PEDIA로 변재일 생년월일을 알고 싶어  \n",
       "2     남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야  \n",
       "3           삼양터널의 총 길이 위키백과사전에서 뭐야  \n",
       "4                코니 윌리스의 태어난 곳은 뭐지  \n",
       "5    WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐  \n",
       "6         WIKIPEDIA백과로 라이프 찾아서 말해줘  \n",
       "7      송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야  \n",
       "8         검색 HLKVAM 언제 출생했는지를 검색해라  \n",
       "9           위키 피디아에 푸조 508 전고가 몇이야  \n",
       "10                 검색 호몬혼 섬 길이를 찾아  \n",
       "11              영산중학교 좀 위키피디아사전 검색  \n",
       "12             위키백과로 침보라조 산 나이 어떤지  \n",
       "13                 마무드 아스라의 출생 찾아줘  \n",
       "14          위키 백과 조제 카리오카의 출생지를 찾아  \n",
       "15              검색 제이 개츠비 생년월일은 뭐지  \n",
       "16           증약터널의 길이가 얼마쯤인지 혹시 알아  \n",
       "17  리히텐슈타인의 한스 아담 2세 소속사는 어딘지 검색해봐  \n",
       "18      WIKI사전백과 검색 벨록스여우의 높이는 얼만지  \n",
       "19                 파블롭스키구의 나이를 찾아줘  \n",
       "20           사카피솔라 섬의 너비는 WIKI에서 뭐  \n",
       "21             나미는 태어난 곳이 WIKI로 뭔지  \n",
       "22               위키에서 피니스테르의 무게 찾기  \n",
       "23           검색 카를 야스퍼스 출신지역이 어디라고  \n",
       "24              63식 병력수송장갑차의 폭 얼만지  \n",
       "25            검색으로 강마에가 출생 장소를 찾아줘  \n",
       "26            쿠죠 히카리의 언제 출생했는지 탐색해  \n",
       "27                  사하라의 길이가 얼마쯤이지  \n",
       "28                   송대산성의 면적은 얼만지  \n",
       "29         WIKI 피디아에 신자경선생묘의 넓이 뭔지  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [(s, i, e) for i,e,s in zip(train_raw['intent'], train_raw['entity'], train_raw['sentence'])]\n",
    "valid_dataset = [(s, i, e) for i,e,s in zip(validation_raw['intent'], validation_raw['entity'], validation_raw['sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('자강의 면적은 얼마 정도되는지 알려줄래', 'area', 'EECCCCCCCCCCCCCCCCCCC')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32\n",
    "\n",
    "length_clip = nlp.data.PadSequence(seq_len, pad_val=\"<pad>\")\n",
    "\n",
    "def preprocess(data):\n",
    "    sent, intent, entity = data\n",
    "    char_sent = list(str(sent))\n",
    "    char_entity = list(str(entity))\n",
    "    char_intent = str(intent)\n",
    "    return(length_clip(char_sent), len(sent), char_intent, length_clip(char_entity))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
    "          .format(end - start, len(dataset)))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=0.24s, #Sentences=9000\n",
      "Done! Tokenizing Time=0.25s, #Sentences=1000\n"
     ]
    }
   ],
   "source": [
    "train_preprocessed  = preprocess_dataset(train_dataset)\n",
    "valid_preprocessed  = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_sent   = nlp.data.count_tokens(itertools.chain.from_iterable([c for c, _, _, _ in train_preprocessed]))\n",
    "counter_intent = nlp.data.count_tokens([c for _,_,c,_ in train_preprocessed])\n",
    "counter_entity = nlp.data.count_tokens(itertools.chain.from_iterable([c for _,_,_,c in train_preprocessed]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'age': 900,\n",
       "         'area': 900,\n",
       "         'belong_to': 900,\n",
       "         'birth_date': 900,\n",
       "         'birth_place': 900,\n",
       "         'definition': 900,\n",
       "         'height': 900,\n",
       "         'length': 900,\n",
       "         'weight': 900,\n",
       "         'width': 900})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sent = nlp.Vocab(counter_sent, bos_token=None, eos_token=None, min_freq=15)\n",
    "vocab_intent = nlp.Vocab(counter_intent, bos_token=None, eos_token=None, unknown_token=None, padding_token=None)\n",
    "vocab_entity = nlp.Vocab(counter_entity, bos_token=None, eos_token=None, unknown_token=None , padding_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>', '<pad>', ' ', 'I', '이', '색', '검', '의', '지', '아'],\n",
       " ['C', '<pad>', 'E'],\n",
       " ['age',\n",
       "  'area',\n",
       "  'belong_to',\n",
       "  'birth_date',\n",
       "  'birth_place',\n",
       "  'definition',\n",
       "  'height',\n",
       "  'length',\n",
       "  'weight',\n",
       "  'width'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sent.idx_to_token[:10], vocab_entity.idx_to_token[:10], vocab_intent.idx_to_token[:10], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed_encoded  = [(vocab_sent[sent], length,vocab_intent[intent] ,vocab_entity[entity] )  \n",
    "                               for sent, length ,intent, entity in train_preprocessed ]\n",
    "valid  = [(vocab_sent[sent], length ,vocab_intent[intent],vocab_entity[entity])  for sent, length , intent,entity in valid_preprocessed ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = nlp.data.train_valid_split(train_preprocessed_encoded, valid_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 30\n",
    "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Stack(),\n",
    "                                      nlp.data.batchify.Stack('float32'),\n",
    "                                      nlp.data.batchify.Stack(),\n",
    "                                      nlp.data.batchify.Stack())\n",
    "\n",
    "train_dataloader  = gluon.data.DataLoader(train, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
    "test_dataloader  = gluon.data.DataLoader(test, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
    "valid_dataloader  = gluon.data.DataLoader(valid, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityTagger_IntentClassification(gluon.HybridBlock):\n",
    "    def __init__(self, vocab_size, intent_class_size, entity_class_size, num_embed, seq_len, hidden_size, **kwargs):\n",
    "        super(EntityTagger_IntentClassification, self).__init__(**kwargs)\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size \n",
    "        self.intent_class_size = intent_class_size\n",
    "        self.entity_class_size = entity_class_size\n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(input_dim=vocab_size, output_dim=num_embed)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "            self.bilstm = rnn.LSTM(self.hidden_size, dropout=0.3)\n",
    "            self.out_intent =nn.Dense(self.intent_class_size)\n",
    "            self.bilstm_last = rnn.LSTM(self.hidden_size, dropout=0.3, bidirectional=True)\n",
    "            \n",
    "            #self.bigru = rnn.GRU(self.hidden_size, dropout=0.2, bidirectional=True)\n",
    "            self.dense_en = nn.Dense(50, flatten=False)\n",
    "            self.out_entity = nn.Dense(self.entity_class_size, flatten=False)\n",
    "            #self.dense = nn.Dense(self.vocab_out_size, flatten=False)        \n",
    "                        \n",
    "    def hybrid_forward(self, F ,inputs, length):\n",
    "        em_out = self.embed(inputs)\n",
    "        #bigruout = self.bigru(em_out)\n",
    "        bilstm = self.bilstm(em_out)\n",
    "        masked_encoded_intent = F.SequenceMask(bilstm,\n",
    "                                        sequence_length=length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_intent = F.broadcast_div(F.sum(masked_encoded_intent, axis=0), F.expand_dims(length, axis=1))\n",
    "        intent = self.out_intent(agg_intent)\n",
    "        \n",
    "        bilstm_last = self.bilstm_last(em_out)        \n",
    "        masked_encoded_entity = F.SequenceMask(bilstm_last,\n",
    "                                        sequence_length=length,\n",
    "                                        use_sequence_length=True).transpose((1,0,2))\n",
    "        fc_entity = self.dense_en(masked_encoded_entity)\n",
    "        entity = self.out_entity(fc_entity) \n",
    "        return(intent, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "\n",
    "model = EntityTagger_IntentClassification(vocab_size = len(vocab_sent.idx_to_token), \n",
    "                                          entity_class_size=len(vocab_entity.idx_to_token),\n",
    "                                          intent_class_size=len(vocab_intent.idx_to_token),\n",
    "                                          num_embed=50, seq_len=seq_len, hidden_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.initializer.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(),\"Adam\")\n",
    "loss = gluon.loss.SoftmaxCELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EntityTagger_IntentClassification(\n",
       "  (out_intent): Dense(None -> 10, linear)\n",
       "  (out_entity): Dense(None -> 3, linear)\n",
       "  (bilstm_last): LSTM(None -> 30, TNC, dropout=0.3, bidirectional)\n",
       "  (embed): Embedding(481 -> 50, float32)\n",
       "  (dropout): Dropout(p = 0.3, axes=())\n",
       "  (bilstm): LSTM(None -> 30, TNC, dropout=0.3)\n",
       "  (dense_en): Dense(None -> 50, linear)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    corrected = 0\n",
    "    n = 0\n",
    "    acc_intent = mx.metric.Accuracy()\n",
    "    for i, (data, length, intent, entity) in enumerate(data_iter):\n",
    "        data = data.as_in_context(ctx)\n",
    "        intent = intent.as_in_context(ctx)\n",
    "        entity = entity.as_in_context(ctx)\n",
    "        length = length.as_in_context(ctx)\n",
    "        intent_output, entity_output = model(data.T, length)\n",
    "        intent_predictions = nd.argmax(intent_output, axis=1)\n",
    "        acc_intent.update(preds=intent_predictions, labels=intent)        \n",
    "        entity_predictions = nd.argmax(entity_output, axis=2) \n",
    "        tf = entity_predictions.astype('int64') == entity\n",
    "        for i in range(length.shape[0]):\n",
    "            l = int(length[i].asscalar())\n",
    "            corrected += nd.sum(tf[i][:l]).asscalar() == l\n",
    "            n += 1\n",
    "    return(corrected/n), (acc_intent.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx=ctx):\n",
    "    intent_loss = []\n",
    "    entity_loss = []\n",
    "    for i, (te_data, te_length, te_intent, te_entity) in enumerate(data_iter):\n",
    "        te_data = te_data.as_in_context(ctx)\n",
    "        te_intent = te_intent.as_in_context(ctx)\n",
    "        te_entity = te_entity.as_in_context(ctx)\n",
    "        te_length = te_length.as_in_context(ctx)\n",
    "        intent_output, entity_output = model(te_data.T, te_length)\n",
    "        \n",
    "        loss_intent = loss_obj(intent_output, te_intent)\n",
    "        curr_loss_intent = nd.mean(loss_intent).asscalar()\n",
    "        intent_loss.append(curr_loss_intent)\n",
    "        \n",
    "        loss_entity = loss_obj(entity_output, te_entity)\n",
    "        curr_loss_entity = nd.mean(loss_entity).asscalar()\n",
    "        entity_loss.append(curr_loss_entity)\n",
    "    return(np.mean(intent_loss), np.mean(entity_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 266.13it/s]\n",
      " 11%|█         | 30/270 [00:00<00:00, 291.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: (0.35134175, 0.02359023), Test Loss : (0.40839735, 0.027739711), Test Accuracy : (0.8233333333333334, 0.9366666666666666), Train Accuracy : (0.8524691358024692, 0.9637037037037037) : Valid Accuracy : (0.748, 0.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 292.67it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 274.60it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 283.13it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 274.78it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 282.03it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 280.42it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 277.56it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 272.10it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 281.00it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 284.94it/s]\n",
      " 11%|█         | 29/270 [00:00<00:00, 288.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: (0.021320323, 0.0019323159), Test Loss : (0.11305806, 0.007870059), Test Accuracy : (0.9633333333333334, 0.9777777777777777), Train Accuracy : (0.9861728395061728, 0.9996296296296296) : Valid Accuracy : (0.901, 0.824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 292.07it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.43it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 292.04it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 292.67it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 296.06it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 291.24it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 288.33it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.72it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 289.58it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 288.87it/s]\n",
      " 11%|█         | 29/270 [00:00<00:00, 284.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20. Train Loss: (0.0046512173, 0.001162052), Test Loss : (0.09036876, 0.00788207), Test Accuracy : (0.9588888888888889, 0.9811111111111112), Train Accuracy : (0.9872839506172839, 1.0) : Valid Accuracy : (0.893, 0.822)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 287.70it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 285.75it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 294.69it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 295.46it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 297.20it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 295.11it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.48it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 289.62it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.08it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 292.50it/s]\n",
      " 11%|█         | 30/270 [00:00<00:00, 293.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30. Train Loss: (0.0021207987, 6.422827e-05), Test Loss : (0.06327557, 0.007953479), Test Accuracy : (0.9722222222222222, 0.9844444444444445), Train Accuracy : (0.9997530864197531, 1.0) : Valid Accuracy : (0.939, 0.835)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 291.75it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 286.78it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.20it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 291.99it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.05it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.06it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 295.29it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 294.93it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.22it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.22it/s]\n",
      " 11%|█         | 29/270 [00:00<00:00, 289.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40. Train Loss: (0.0005918092, 4.9288956e-06), Test Loss : (0.12045789, 0.010654632), Test Accuracy : (0.97, 0.9766666666666667), Train Accuracy : (1.0, 1.0) : Valid Accuracy : (0.933, 0.822)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 285.58it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 291.53it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.84it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 292.18it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.04it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 288.48it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.66it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 291.48it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 293.22it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 288.60it/s]\n",
      " 11%|█         | 29/270 [00:00<00:00, 284.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50. Train Loss: (0.0020211546, 0.00012501831), Test Loss : (0.07394106, 0.008759798), Test Accuracy : (0.9744444444444444, 0.9822222222222222), Train Accuracy : (0.9988888888888889, 0.9993827160493827) : Valid Accuracy : (0.942, 0.844)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 288.03it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.78it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.30it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 288.83it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 292.02it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 294.62it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 296.39it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 295.70it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 295.36it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.36it/s]\n",
      " 11%|█         | 29/270 [00:00<00:00, 282.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60. Train Loss: (0.00021131491, 3.6445895e-06), Test Loss : (0.09383799, 0.010895292), Test Accuracy : (0.9777777777777777, 0.9777777777777777), Train Accuracy : (1.0, 1.0) : Valid Accuracy : (0.939, 0.842)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 283.94it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.93it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.27it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 288.66it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 286.94it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.31it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 285.40it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.26it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.25it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.72it/s]\n",
      " 10%|█         | 28/270 [00:00<00:00, 273.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70. Train Loss: (8.8357236e-05, 7.0657944e-07), Test Loss : (0.13797253, 0.013243537), Test Accuracy : (0.9755555555555555, 0.9755555555555555), Train Accuracy : (1.0, 1.0) : Valid Accuracy : (0.938, 0.828)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 281.14it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 286.65it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 282.98it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 289.75it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 290.31it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 285.65it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 291.05it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 293.80it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 294.64it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 286.82it/s]\n",
      " 11%|█         | 29/270 [00:00<00:00, 283.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80. Train Loss: (2.9432895e-05, 1.3592586e-07), Test Loss : (0.17839178, 0.016036982), Test Accuracy : (0.9744444444444444, 0.9744444444444444), Train Accuracy : (1.0, 1.0) : Valid Accuracy : (0.936, 0.823)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:00<00:00, 286.43it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 251.95it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 293.48it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.21it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 265.63it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 287.14it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 292.87it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 286.61it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 267.91it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 260.14it/s]\n",
      "  9%|▉         | 24/270 [00:00<00:01, 230.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90. Train Loss: (0.000113624585, 9.535835e-06), Test Loss : (0.06316081, 0.008536507), Test Accuracy : (0.9744444444444444, 0.9855555555555555), Train Accuracy : (1.0, 1.0) : Valid Accuracy : (0.918, 0.835)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 270/270 [00:01<00:00, 256.62it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 283.31it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 269.47it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 284.30it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 274.53it/s]\n",
      "100%|██████████| 270/270 [00:00<00:00, 274.23it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 255.14it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 257.04it/s]\n",
      "100%|██████████| 270/270 [00:01<00:00, 254.27it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_train_loss = []\n",
    "tot_test_accu = []\n",
    "tot_train_accu = []\n",
    "tot_valid_accu = [] \n",
    "\n",
    "for e in range(epochs):\n",
    "    #batch training \n",
    "    for i, (data, length, intent, entity) in enumerate(tqdm(train_dataloader)):\n",
    "        data = data.as_in_context(ctx)\n",
    "        intent = intent.as_in_context(ctx)\n",
    "        entity = entity.as_in_context(ctx)\n",
    "        length = length.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            out_intent, out_entity = model(data.T, length)\n",
    "            loss_intent = loss(out_intent, intent)\n",
    "            loss_entity = loss(out_entity, entity)\n",
    "            loss_ = loss_intent * 0.4 + loss_entity * 0.6\n",
    "            loss_.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "    #caculate test loss\n",
    "    if e % 10 == 0: \n",
    "        test_loss = calculate_loss(model, test_dataloader, loss_obj = loss, ctx=ctx) \n",
    "        train_loss = calculate_loss(model, train_dataloader, loss_obj = loss, ctx=ctx)\n",
    "        test_accu = evaluate_accuracy(model, test_dataloader,  ctx=ctx)\n",
    "        train_accu = evaluate_accuracy(model, train_dataloader,  ctx=ctx)\n",
    "        valid_accu = evaluate_accuracy(model, valid_dataloader,  ctx=ctx)\n",
    "        print(\"Epoch %s. Train Loss: %s, Test Loss : %s,\" \\\n",
    "        \" Test Accuracy : %s,\" \\\n",
    "        \" Train Accuracy : %s : Valid Accuracy : %s\" % (e, train_loss, test_loss, test_accu, train_accu, valid_accu))    \n",
    "        tot_test_loss.append(test_loss)\n",
    "        tot_train_loss.append(train_loss)\n",
    "        tot_test_accu.append(test_accu)\n",
    "        tot_train_accu.append(train_accu)\n",
    "        tot_valid_accu.append(valid_accu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model export and Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netron으로 네트워크 시각화 \n",
    "\n",
    "- https://lutzroeder.github.io/netron/\n",
    "- 저장된 `model-symbol.json`을 입력해 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = gluon.nn.SymbolBlock.imports(\"model-symbol.json\", ['data0', 'data1'], \"model-0000.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entitytag(sent):\n",
    "    sent_len = len(sent)\n",
    "    coded_sent = vocab_sent[length_clip(sent)]\n",
    "    co = nd.array(coded_sent).expand_dims(axis=1)\n",
    "    ret_code = load_model(co, nd.array([sent_len,]))\n",
    "    ret_seq = vocab_entity.to_tokens(ret_code.argmax(axis=2)[0].asnumpy().astype('int').tolist())\n",
    "    return(''.join(ret_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Test Accuracy 95% 이상 올리기\n",
    "- test_hidden 셋의 성능 90% 이상 올리기 \n",
    "- Entity Tagging과 Intent Classification을 MultiTask Learning으로 통합해보기(성능이 좋아지나? 나빠지나?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## entity tagging\n",
    "Epoch 90. Train Loss: 7.2252215e-10, Test Loss : 0.01954503, Test Accuracy : 0.9722222222222222, Train Accuracy : 1.0 : Valid Accuracy : 0.976\n",
    "\n",
    "## intent classification \n",
    "Epoch 90. Train Loss: 2.0868972e-08, Test Loss : 0.033646293, Test Accuracy : 0.9966666666666667, Train Accuracy : 1.0 : Valid Accuracy : 0.992\n",
    "\n",
    "## multi-task-learning\n",
    "Epoch 90. Train Loss: (0.000113624585, 9.535835e-06), Test Loss : (0.06316081, 0.008536507), Test Accuracy : (0.9744444444444444, 0.9855555555555555), Train Accuracy : (1.0, 1.0) : Valid Accuracy : (0.918, 0.835)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
