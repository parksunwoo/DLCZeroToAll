{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 무비 리뷰 분류 모형 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "약 2만건의 네이버 무비 리뷰 데이터를 활용해 Sentiment Classification을 하는 모형을 만들어 본다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd \n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "mecab = Mecab()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab 생성 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습셋 전체의 문장을 이용해 전처리를 한 뒤, Vocab을 생성한다. `Mecab` 형태소 분석기로 형태소만으로 Vocab을 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = pd.read_csv(\"ratings.txt\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [(d, l) for d,l in zip(rating['document'], rating['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_clip = nlp.data.PadSequence(seq_len, pad_val=\"<pad>\")\n",
    "\n",
    "#import re\n",
    "#REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "#REPLACE_WITH_SPACE = re.compile('<')\n",
    "\n",
    "def preprocess(data):\n",
    "    comment, label = data\n",
    "    # 형태소 기준으로 전처리\n",
    "    morphs = mecab.morphs(str(comment).strip())\n",
    "    return(length_clip(morphs), label)\n",
    "    \n",
    "    # 명사 기준으로 전처리 변경1 -> 74%\n",
    "    # nouns = mecab.nouns(str(comment).strip()) \n",
    "    # return (length_clip(nouns), label)\n",
    "    \n",
    "    # 품사 기준으로 전처리 변경2 -> TypeError: '<' not supported between instances of 'str' and 'tuple' \n",
    "    #comment = [REPLACE_WITH_SPACE.sub(\" \", line) for line in comment]\n",
    "    #pos = mecab.pos(str(comment).strip()) \n",
    "    #return (length_clip(pos), label)\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        #lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
    "          .format(end - start, len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=6.70s, #Sentences=200000\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocess_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 문장의 첫 11개 토큰 출력  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대박', '이', '였', '지', '이건', '한마디', '로', '.', '.', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[100][0][:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습셋 전체로 토큰 빈도를 생성 `counter`를 만들고, `vocab`을 생성. \n",
    "문장 생성이나 seq2seq가 아니기 때문에 `bos_token`, `eos_token` 표현은 생략 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable([c for c, _ in preprocessed]))\n",
    "#vocab = nlp.Vocab(counter,bos_token=None, eos_token=None, min_freq=15)\n",
    "vocab = nlp.Vocab(counter,bos_token=None, eos_token=None, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_embedding(fasttext_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습셋 생성 \n",
    "\n",
    "토큰을 `index`로 변환 하여 학습을 위한 데이터로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_encoded  = [(vocab[data], label)  for data, label in preprocessed ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = nlp.data.train_valid_split(preprocessed_encoded, valid_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Stack(),\n",
    "                                      nlp.data.batchify.Stack('float32'))\n",
    "\n",
    "train_dataloader  = gluon.data.DataLoader(train, batch_size=100, batchify_fn=batchify_fn, shuffle=True, last_batch='discard')\n",
    "test_dataloader  = gluon.data.DataLoader(test, batch_size=100, batchify_fn=batchify_fn, shuffle=True, last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentClassificationModelAtt(gluon.HybridBlock):\n",
    "    def __init__(self, vocab_size, num_embed, hidden_size, **kwargs):\n",
    "        super(SentClassificationModelAtt, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(input_dim=vocab_size, output_dim=num_embed)\n",
    "            self.drop = nn.Dropout(0.3)\n",
    "            self.lstm = rnn.LSTM(self.hidden_size, dropout=0.2, bidirectional=True, layout=\"NTC\")\n",
    "            self.attention = nlp.model.MLPAttentionCell(30, dropout=0.2)\n",
    "            #self.fc = nn.Dense(100)\n",
    "            self.dense = nn.Dense(2)\n",
    "            \n",
    "    def hybrid_forward(self, F ,inputs):\n",
    "        em_out = self.drop(self.embed(inputs))\n",
    "        bilstm_out = self.lstm(em_out)\n",
    "        ctx_vector, _ = self.attention(bilstm_out, bilstm_out)\n",
    "        outs = self.dense(ctx_vector)\n",
    "        return(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "#ctx = mx.cpu()\n",
    "\n",
    "#모형 인스턴스 생성 및 트래이너, loss 정의 \n",
    "model = SentClassificationModelAtt(vocab_size = len(vocab.idx_to_token), num_embed=vocab.embedding.idx_to_vec.shape[1],\n",
    "                                  hidden_size=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(),ctx=ctx)\n",
    "#pre-trained 된 임베딩은 모델 초기화 이후 적용한다.\n",
    "model.embed.weight.set_data(vocab.embedding.idx_to_vec.as_in_context(ctx))\n",
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'adam')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iter):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return(acc.get()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx=ctx):\n",
    "    test_loss = []\n",
    "    for i, (te_data, te_label) in enumerate(data_iter):\n",
    "        te_data = te_data.as_in_context(ctx)\n",
    "        te_label = te_label.as_in_context(ctx)\n",
    "        te_output = model(te_data)\n",
    "        loss_te = loss_obj(te_output, te_label)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return(np.mean(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:50<00:00, 35.99it/s]\n",
      "  0%|          | 0/1800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.3649794, Test Loss : 0.32480225, Test Accuracy : 0.8605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:49<00:00, 36.14it/s]\n",
      "  0%|          | 5/1800 [00:00<00:37, 47.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train Loss: 0.29139355, Test Loss : 0.30886558, Test Accuracy : 0.8652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:49<00:00, 36.47it/s]\n",
      "  0%|          | 5/1800 [00:00<00:37, 47.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2. Train Loss: 0.25976864, Test Loss : 0.3031374, Test Accuracy : 0.87005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:33<00:00, 53.36it/s]\n",
      "  0%|          | 6/1800 [00:00<00:30, 59.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3. Train Loss: 0.2343237, Test Loss : 0.32367703, Test Accuracy : 0.8704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:29<00:00, 61.41it/s]\n",
      "  0%|          | 6/1800 [00:00<00:30, 59.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4. Train Loss: 0.20986067, Test Loss : 0.3312329, Test Accuracy : 0.8678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:29<00:00, 61.46it/s]\n",
      "  0%|          | 6/1800 [00:00<00:30, 59.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5. Train Loss: 0.18674502, Test Loss : 0.34211674, Test Accuracy : 0.8672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:29<00:00, 61.41it/s]\n",
      "  0%|          | 6/1800 [00:00<00:30, 59.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6. Train Loss: 0.16510928, Test Loss : 0.36338326, Test Accuracy : 0.86695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:29<00:00, 61.40it/s]\n",
      "  0%|          | 6/1800 [00:00<00:30, 59.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7. Train Loss: 0.14514303, Test Loss : 0.4163906, Test Accuracy : 0.8622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:29<00:00, 61.16it/s]\n",
      "  0%|          | 6/1800 [00:00<00:30, 59.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8. Train Loss: 0.1281185, Test Loss : 0.44837943, Test Accuracy : 0.86255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:29<00:00, 61.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9. Train Loss: 0.112834066, Test Loss : 0.51143116, Test Accuracy : 0.8595\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_test_accu = []\n",
    "tot_train_loss = []\n",
    "for e in range(epochs):\n",
    "    train_loss = []\n",
    "    #batch training \n",
    "    for i, (data, label) in enumerate(tqdm(train_dataloader)):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        \n",
    "        ###print(data.shape)\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            loss_ = loss(output, label)\n",
    "            loss_.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        curr_loss = nd.mean(loss_).asscalar()\n",
    "        train_loss.append(curr_loss)\n",
    "\n",
    "    #caculate test loss\n",
    "    test_loss = calculate_loss(model, test_dataloader, loss_obj = loss, ctx=ctx) \n",
    "    test_accu = evaluate_accuracy(model, test_dataloader,  ctx=ctx)\n",
    "\n",
    "    print(\"Epoch %s. Train Loss: %s, Test Loss : %s, Test Accuracy : %s\" % (e, np.mean(train_loss), test_loss, test_accu))    \n",
    "    tot_test_loss.append(test_loss)\n",
    "    tot_train_loss.append(np.mean(train_loss))\n",
    "    tot_test_accu.append(test_accu)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "\n",
    "- 테스트 정확도를 87% 이상 올려본다.(Optimizer, RNN, Convolution, 데이터 전처리 방식 변경(명사만 사용?), ...) \n",
    "- 학습된 임베딩 레이어를 기반으로 단어간의 유사도를 구해본다. \n",
    "- 토큰이 아닌 char 기반으로 학습하면 어떨까? 성능이 좋아지나? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 명사 기준으로 전처리 변경1 -> 74%\n",
    "    # nouns = mecab.nouns(str(comment).strip()) \n",
    "    # return (length_clip(nouns), label)\n",
    "    \n",
    "    # 품사 기준으로 전처리 변경2 -> TypeError: '<' not supported between instances of 'str' and 'tuple' \n",
    "    #comment = [REPLACE_WITH_SPACE.sub(\" \", line) for line in comment]\n",
    "    #pos = mecab.pos(str(comment).strip()) \n",
    "    #return (length_clip(pos), label)\n",
    "\n",
    "## optimizer sgd로 변경1 -> 81%\n",
    "    # trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': .1})\n",
    "    \n",
    "    # optimizer sgd로 변경2 -> 50%?????\n",
    "    # args_lr = 1.0\n",
    "    # trainer = gluon.Trainer(model.collect_params(),'sgd', {'learning_rate': args_lr, 'momentum': 0, 'wd': 0})\n",
    "\n",
    "## fasttext add -> 84% 그대로\n",
    "fasttext_simple = nlp.embedding.create('fasttext', source='wiki.ko', load_ngrams=True)\n",
    "vocab.set_embedding(fasttext_simple)\n",
    "\n",
    "## Sentiment Analysis (SA) with pre-trained Language Model (LM) -> 87%\n",
    "[Epoch 0] train avg loss 0.001483, test acc 0.87, test avg loss 0.313094, throughput 9.40K wps\n",
    "\n",
    "## Sentiment Analysis (SA) with pre-trained Language Model (LM)_ko -> 55%\n",
    "[Epoch 0] train avg loss 0.022161, test acc 0.55, test avg loss 0.660891, throughput 4.59K wps\n",
    "\n",
    "## Sentiment Analysis (SA) with pre-trained Language Model (LM)_ko_sejong_dataset -> 81%\n",
    "[Epoch 0] train avg loss 0.013356, test acc 0.81, test avg loss 0.367918, throughput 4.44K wps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp1. vocab을 sejong_dataset으로 변경해서 진행 -> 82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sejong_dataset = nlp.data.dataset.CorpusDataset('KoWordSpacing/input.txt', tokenizer=lambda x:mecab.morphs(x.strip()))\n",
    "#counter = nlp.data.count_tokens(itertools.chain.from_iterable(sejong_dataset))\n",
    "#vocab = nlp.Vocab(counter, unknown_token='<unk>', padding_token=None, bos_token=None, eos_token=None, min_freq=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp2. Attaching word embeddings -> 83%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext_simple = nlp.embedding.create('fasttext', source='wiki.ko', load_ngrams=True)\n",
    "#vocab.set_embedding(fasttext_simple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp3. Creating Vocabulary from Pre-trained Word Embeddings -> 55%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_6b50d = nlp.embedding.create('glove', source='glove.6B.50d')\n",
    "#vocab = nlp.Vocab(nlp.data.Counter(glove_6b50d.idx_to_token))\n",
    "#vocab.set_embedding(glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp4. skit learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ratings.txt\", sep='\\t', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1  8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2  4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "text = df['document'].as_matrix()\n",
    "y = df['label'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_train, text_test, y_train, y_test = train_test_split(text, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter_tag = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_tokenizer(text):\n",
    "    return twitter_tag.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mecab_tokenizer(text):\n",
    "    return mecab.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_param_grid = {'tfidfvectorizer__min_df':[3,5,7],\n",
    "                   'tfidfvectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "                    'logisticregression__C':[0.1, 1, 10, 100]}\n",
    "mecab_pipe = make_pipeline(TfidfVectorizer(tokenizer=mecab_tokenizer), LogisticRegression())\n",
    "mecab_grid = GridSearchCV(mecab_pipe, mecab_param_grid, n_jobs=-1)\n",
    "\n",
    "mecab_grid.fit(text_train, y_train)\n",
    "print(\"최상의 교차 검증 점수 {:.3f}\".format(mecab_grid.best_score_))\n",
    "print(\"최적의 교차 검증 매개변수 \", mecab_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mecab = mecab_grid.best_estimator_.named_steps['tfidfvectorizer'].transform(text_test)\n",
    "score = mecab_grid.best_estimator_.named_steps['logisticregression'].score(X_test_mecab, y_test)\n",
    "print(\"테스트 세트 점수: :{:.3f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_param_grid = {'tfidfvectorizer__min_df':[3,5,7],\n",
    "                   'tfidfvectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "                    'logisticregression__C':[0.1, 1, 10]}\n",
    "twit_pipe = make_pipeline(TfidfVectorizer(tokenizer=twitter_tokenizer), LogisticRegression())\n",
    "twit_grid = GridSearchCV(twit_pipe, twit_param_grid, n_jobs=-1)\n",
    "\n",
    "twit_grid.fit(text_train[0:1000], y_train[0:1000])\n",
    "print(\"최상의 교차 검증 점수: {:.3f}\".format(twit_grid.best_score_))\n",
    "print(\"최적의 교차 검증 매개변수 \", twit_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_knolpy = grid.best_estimator_.named_steps['tfidfvectorizer'].transform(text_test)\n",
    "score = grid.best_estimator_.named_steps['logisticregression'].score(X_test_knolpy, y_test)\n",
    "print(\"테스트 세트 점수 : {:.3f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
