{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Taggging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "from mxnet import nd \n",
    "import mxnet as mx\n",
    "import time\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"data/trainset.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
    "validation_raw = pd.read_csv(\"data/test_hidden.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')\n",
    "#validation_raw = pd.read_csv(\"data/validation.txt\",names=['intent', 'entity', 'sentence'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intent</th>\n",
       "      <th>entity</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>area</td>\n",
       "      <td>EECCCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>자강의 면적은 얼마 정도되는지 알려줄래</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCCCCCCCCCCEEECCCCCCCCCCCC</td>\n",
       "      <td>WIKI PEDIA로 변재일 생년월일을 알고 싶어</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>EEEEEEEEEEECCCCCCCCCCCCCCCCC</td>\n",
       "      <td>남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>length</td>\n",
       "      <td>EEEECCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>삼양터널의 총 길이 위키백과사전에서 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>EEEEEECCCCCCCCCCC</td>\n",
       "      <td>코니 윌리스의 태어난 곳은 뭐지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weight</td>\n",
       "      <td>CCCCCCCCCCCCEEEECCCCCCCCCCCCC</td>\n",
       "      <td>WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>definition</td>\n",
       "      <td>CCCCCCCCCCCCCEEECCCCCCCC</td>\n",
       "      <td>WIKIPEDIA백과로 라이프 찾아서 말해줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>height</td>\n",
       "      <td>EEEEEEEECCCCCCCCCCCCCCCCCCC</td>\n",
       "      <td>송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCEEEEEECCCCCCCCCCCCCCC</td>\n",
       "      <td>검색 HLKVAM 언제 출생했는지를 검색해라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>height</td>\n",
       "      <td>CCCCCCCCEEEEEECCCCCCCC</td>\n",
       "      <td>위키 피디아에 푸조 508 전고가 몇이야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>length</td>\n",
       "      <td>CCCEEEEECCCCCCC</td>\n",
       "      <td>검색 호몬혼 섬 길이를 찾아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>definition</td>\n",
       "      <td>EEEEECCCCCCCCCCCCC</td>\n",
       "      <td>영산중학교 좀 위키피디아사전 검색</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>age</td>\n",
       "      <td>CCCCCCEEEEEECCCCCCC</td>\n",
       "      <td>위키백과로 침보라조 산 나이 어떤지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>EEEEEEECCCCCCCC</td>\n",
       "      <td>마무드 아스라의 출생 찾아줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>CCCCCCEEEEEEECCCCCCCCC</td>\n",
       "      <td>위키 백과 조제 카리오카의 출생지를 찾아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>CCCEEEEEECCCCCCCCC</td>\n",
       "      <td>검색 제이 개츠비 생년월일은 뭐지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>length</td>\n",
       "      <td>EEEECCCCCCCCCCCCCCCCC</td>\n",
       "      <td>증약터널의 길이가 얼마쯤인지 혹시 알아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>belong_to</td>\n",
       "      <td>EEEEEEEEEEEEEEEECCCCCCCCCCCCCC</td>\n",
       "      <td>리히텐슈타인의 한스 아담 2세 소속사는 어딘지 검색해봐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>height</td>\n",
       "      <td>CCCCCCCCCCCCEEEEECCCCCCCCC</td>\n",
       "      <td>WIKI사전백과 검색 벨록스여우의 높이는 얼만지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>age</td>\n",
       "      <td>EEEEEECCCCCCCCC</td>\n",
       "      <td>파블롭스키구의 나이를 찾아줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>width</td>\n",
       "      <td>EEEEEEECCCCCCCCCCCCCC</td>\n",
       "      <td>사카피솔라 섬의 너비는 WIKI에서 뭐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>EECCCCCCCCCCCCCCCCC</td>\n",
       "      <td>나미는 태어난 곳이 WIKI로 뭔지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>weight</td>\n",
       "      <td>CCCCCEEEEECCCCCCC</td>\n",
       "      <td>위키에서 피니스테르의 무게 찾기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>CCCEEEEEEECCCCCCCCCCC</td>\n",
       "      <td>검색 카를 야스퍼스 출신지역이 어디라고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>width</td>\n",
       "      <td>EEEEEEEEEEECCCCCCC</td>\n",
       "      <td>63식 병력수송장갑차의 폭 얼만지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>birth_place</td>\n",
       "      <td>CCCCCEEECCCCCCCCCCCC</td>\n",
       "      <td>검색으로 강마에가 출생 장소를 찾아줘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>birth_date</td>\n",
       "      <td>EEEEEECCCCCCCCCCCCCC</td>\n",
       "      <td>쿠죠 히카리의 언제 출생했는지 탐색해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>length</td>\n",
       "      <td>EEECCCCCCCCCCC</td>\n",
       "      <td>사하라의 길이가 얼마쯤이지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>area</td>\n",
       "      <td>EEEECCCCCCCCC</td>\n",
       "      <td>송대산성의 면적은 얼만지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>area</td>\n",
       "      <td>CCCCCCCCCCEEEEEECCCCCCC</td>\n",
       "      <td>WIKI 피디아에 신자경선생묘의 넓이 뭔지</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intent                          entity  \\\n",
       "0          area           EECCCCCCCCCCCCCCCCCCC   \n",
       "1    birth_date     CCCCCCCCCCCCEEECCCCCCCCCCCC   \n",
       "2           age    EEEEEEEEEEECCCCCCCCCCCCCCCCC   \n",
       "3        length          EEEECCCCCCCCCCCCCCCCCC   \n",
       "4   birth_place               EEEEEECCCCCCCCCCC   \n",
       "5        weight   CCCCCCCCCCCCEEEECCCCCCCCCCCCC   \n",
       "6    definition        CCCCCCCCCCCCCEEECCCCCCCC   \n",
       "7        height     EEEEEEEECCCCCCCCCCCCCCCCCCC   \n",
       "8    birth_date        CCCEEEEEECCCCCCCCCCCCCCC   \n",
       "9        height          CCCCCCCCEEEEEECCCCCCCC   \n",
       "10       length                 CCCEEEEECCCCCCC   \n",
       "11   definition              EEEEECCCCCCCCCCCCC   \n",
       "12          age             CCCCCCEEEEEECCCCCCC   \n",
       "13   birth_date                 EEEEEEECCCCCCCC   \n",
       "14  birth_place          CCCCCCEEEEEEECCCCCCCCC   \n",
       "15   birth_date              CCCEEEEEECCCCCCCCC   \n",
       "16       length           EEEECCCCCCCCCCCCCCCCC   \n",
       "17    belong_to  EEEEEEEEEEEEEEEECCCCCCCCCCCCCC   \n",
       "18       height      CCCCCCCCCCCCEEEEECCCCCCCCC   \n",
       "19          age                 EEEEEECCCCCCCCC   \n",
       "20        width           EEEEEEECCCCCCCCCCCCCC   \n",
       "21  birth_place             EECCCCCCCCCCCCCCCCC   \n",
       "22       weight               CCCCCEEEEECCCCCCC   \n",
       "23  birth_place           CCCEEEEEEECCCCCCCCCCC   \n",
       "24        width              EEEEEEEEEEECCCCCCC   \n",
       "25  birth_place            CCCCCEEECCCCCCCCCCCC   \n",
       "26   birth_date            EEEEEECCCCCCCCCCCCCC   \n",
       "27       length                  EEECCCCCCCCCCC   \n",
       "28         area                   EEEECCCCCCCCC   \n",
       "29         area         CCCCCCCCCCEEEEEECCCCCCC   \n",
       "\n",
       "                          sentence  \n",
       "0            자강의 면적은 얼마 정도되는지 알려줄래  \n",
       "1      WIKI PEDIA로 변재일 생년월일을 알고 싶어  \n",
       "2     남쪽 물고기자리 알파 나이가 위키백과사전으로 얼마야  \n",
       "3           삼양터널의 총 길이 위키백과사전에서 뭐야  \n",
       "4                코니 윌리스의 태어난 곳은 뭐지  \n",
       "5    WIKI백과사전 검색 AA12의 무게가 얼만지 찾아봐  \n",
       "6         WIKIPEDIA백과로 라이프 찾아서 말해줘  \n",
       "7      송파 헬리오시티 구조물 높이 위키 피디아에서 뭐야  \n",
       "8         검색 HLKVAM 언제 출생했는지를 검색해라  \n",
       "9           위키 피디아에 푸조 508 전고가 몇이야  \n",
       "10                 검색 호몬혼 섬 길이를 찾아  \n",
       "11              영산중학교 좀 위키피디아사전 검색  \n",
       "12             위키백과로 침보라조 산 나이 어떤지  \n",
       "13                 마무드 아스라의 출생 찾아줘  \n",
       "14          위키 백과 조제 카리오카의 출생지를 찾아  \n",
       "15              검색 제이 개츠비 생년월일은 뭐지  \n",
       "16           증약터널의 길이가 얼마쯤인지 혹시 알아  \n",
       "17  리히텐슈타인의 한스 아담 2세 소속사는 어딘지 검색해봐  \n",
       "18      WIKI사전백과 검색 벨록스여우의 높이는 얼만지  \n",
       "19                 파블롭스키구의 나이를 찾아줘  \n",
       "20           사카피솔라 섬의 너비는 WIKI에서 뭐  \n",
       "21             나미는 태어난 곳이 WIKI로 뭔지  \n",
       "22               위키에서 피니스테르의 무게 찾기  \n",
       "23           검색 카를 야스퍼스 출신지역이 어디라고  \n",
       "24              63식 병력수송장갑차의 폭 얼만지  \n",
       "25            검색으로 강마에가 출생 장소를 찾아줘  \n",
       "26            쿠죠 히카리의 언제 출생했는지 탐색해  \n",
       "27                  사하라의 길이가 얼마쯤이지  \n",
       "28                   송대산성의 면적은 얼만지  \n",
       "29         WIKI 피디아에 신자경선생묘의 넓이 뭔지  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [(l, d) for d,l in zip(train_raw['entity'], train_raw['sentence'])]\n",
    "valid_dataset = [(l, d) for d,l in zip(validation_raw['entity'], validation_raw['sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 32\n",
    "\n",
    "length_clip = nlp.data.PadSequence(seq_len, pad_val=\"<pad>\")\n",
    "\n",
    "def preprocess(data):\n",
    "    sent, entity = data\n",
    "    char_sent = list(str(sent))\n",
    "    char_entity = list(str(entity))\n",
    "    return(length_clip(char_sent), len(sent),length_clip(char_entity))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'\n",
    "          .format(end - start, len(dataset)))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Tokenizing Time=0.28s, #Sentences=9000\n",
      "Done! Tokenizing Time=0.17s, #Sentences=1000\n"
     ]
    }
   ],
   "source": [
    "train_preprocessed  = preprocess_dataset(train_dataset)\n",
    "valid_preprocessed  = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_sent   = nlp.data.count_tokens(itertools.chain.from_iterable([c for c, _, _ in train_preprocessed]))\n",
    "counter_entity = nlp.data.count_tokens(itertools.chain.from_iterable([c for _,_, c in train_preprocessed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sent = nlp.Vocab(counter_sent, bos_token=None, eos_token=None, min_freq=15)\n",
    "vocab_entity = nlp.Vocab(counter_entity, bos_token=None, eos_token=None, unknown_token=None ,min_freq=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>', '<pad>', ' ', 'I', '이', '색', '검', '의', '지', '아'],\n",
       " ['<pad>', 'C', 'E'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sent.idx_to_token[:10], vocab_entity.idx_to_token[:10], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed_encoded  = [(vocab_sent[sent], length ,vocab_entity[entity])  for sent, length ,entity in train_preprocessed ]\n",
    "valid  = [(vocab_sent[sent], length ,vocab_entity[entity])  for sent, length ,entity in valid_preprocessed ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = nlp.data.train_valid_split(train_preprocessed_encoded, valid_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatch = 30\n",
    "batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Stack(),\n",
    "                                      nlp.data.batchify.Stack('float32'),\n",
    "                                      nlp.data.batchify.Stack())\n",
    "\n",
    "train_dataloader  = gluon.data.DataLoader(train, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
    "test_dataloader  = gluon.data.DataLoader(test, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)\n",
    "valid_dataloader  = gluon.data.DataLoader(valid, batch_size=nbatch, batchify_fn=batchify_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityTagger(gluon.HybridBlock):\n",
    "    def __init__(self, vocab_size, vocab_out_size, num_embed, hidden_size, **kwargs):\n",
    "        super(EntityTagger, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size \n",
    "        self.vocab_out_size = vocab_out_size\n",
    "        with self.name_scope():\n",
    "            self.embed = nn.Embedding(input_dim=vocab_size, output_dim=num_embed)\n",
    "            self.bigru = rnn.GRU(self.hidden_size, dropout=0.2, bidirectional=True)\n",
    "            self.dense_prev = nn.Dense(10, flatten=False)\n",
    "            self.dense = nn.Dense(self.vocab_out_size, flatten=False)  \n",
    "            \n",
    "    def hybrid_forward(self, F ,inputs, length):\n",
    "        em_out = self.embed(inputs)\n",
    "        bigruout = self.bigru(em_out)\n",
    "        masked_encoded = F.SequenceMask(bigruout,\n",
    "                                        sequence_length=length,\n",
    "                                        use_sequence_length=True).transpose((1,0,2))\n",
    "        dense_out = self.dense_prev(masked_encoded)\n",
    "        outs = self.dense(dense_out) \n",
    "        return(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "\n",
    "model = EntityTagger(vocab_size = len(vocab_sent.idx_to_token), vocab_out_size=len(vocab_entity.idx_to_token), \n",
    "                     num_embed=50, hidden_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.initializer.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(),\"Adam\")\n",
    "loss = gluon.loss.SoftmaxCELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EntityTagger(\n",
       "  (embed): Embedding(481 -> 50, float32)\n",
       "  (bigru): GRU(None -> 30, TNC, dropout=0.2, bidirectional)\n",
       "  (dense_prev): Dense(None -> 10, linear)\n",
       "  (dense): Dense(None -> 3, linear)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, data_iter, ctx=ctx):\n",
    "    corrected = 0\n",
    "    n = 0\n",
    "    for i, (data, length, label) in enumerate(data_iter):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        length = length.as_in_context(ctx)\n",
    "        output = model(data.T, length)\n",
    "        predictions = nd.argmax(output, axis=2)\n",
    "        tf = predictions.astype('int64') == label\n",
    "        for i in range(length.shape[0]):\n",
    "            l = int(length[i].asscalar())\n",
    "            corrected += nd.sum(tf[i][:l]).asscalar() == l\n",
    "            n += 1\n",
    "        #acc.update(preds=predictions, labels=label)\n",
    "    return(corrected/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model, data_iter, loss_obj, ctx=ctx):\n",
    "    test_loss = []\n",
    "    for i, (te_data, te_length, te_label) in enumerate(data_iter):\n",
    "        te_data = te_data.as_in_context(ctx)\n",
    "        te_label = te_label.as_in_context(ctx)\n",
    "        te_length = te_length.as_in_context(ctx)\n",
    "        te_output = model(te_data.T, te_length)\n",
    "        loss_te = loss_obj(te_output, te_label)\n",
    "        curr_loss = nd.mean(loss_te).asscalar()\n",
    "        test_loss.append(curr_loss)\n",
    "    return(np.mean(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 7/270 [00:00<00:04, 60.40it/s]\u001b[A\n",
      "  5%|▍         | 13/270 [00:00<00:04, 57.98it/s]\u001b[A\n",
      "  7%|▋         | 19/270 [00:00<00:04, 57.28it/s]\u001b[A\n",
      "  9%|▉         | 25/270 [00:00<00:04, 56.46it/s]\u001b[A\n",
      " 13%|█▎        | 35/270 [00:00<00:03, 63.84it/s]\u001b[A\n",
      " 16%|█▌        | 43/270 [00:00<00:03, 65.45it/s]\u001b[A\n",
      " 19%|█▊        | 50/270 [00:00<00:03, 63.26it/s]\u001b[A\n",
      " 21%|██        | 56/270 [00:00<00:03, 62.18it/s]\u001b[A\n",
      " 23%|██▎       | 62/270 [00:01<00:03, 61.32it/s]\u001b[A\n",
      " 25%|██▌       | 68/270 [00:01<00:03, 60.60it/s]\u001b[A\n",
      " 27%|██▋       | 74/270 [00:01<00:03, 58.36it/s]\u001b[A\n",
      " 30%|██▉       | 80/270 [00:01<00:03, 56.60it/s]\u001b[A\n",
      " 32%|███▏      | 86/270 [00:01<00:03, 56.53it/s]\u001b[A\n",
      " 34%|███▍      | 92/270 [00:01<00:03, 55.47it/s]\u001b[A\n",
      " 36%|███▌      | 97/270 [00:01<00:03, 54.13it/s]\u001b[A\n",
      " 38%|███▊      | 103/270 [00:01<00:03, 54.11it/s]\u001b[A\n",
      " 40%|████      | 109/270 [00:02<00:02, 54.12it/s]\u001b[A\n",
      " 43%|████▎     | 115/270 [00:02<00:02, 54.16it/s]\u001b[A\n",
      " 45%|████▌     | 122/270 [00:02<00:02, 54.62it/s]\u001b[A\n",
      " 49%|████▉     | 132/270 [00:02<00:02, 56.38it/s]\u001b[A\n",
      " 51%|█████▏    | 139/270 [00:02<00:02, 56.57it/s]\u001b[A\n",
      " 54%|█████▍    | 146/270 [00:02<00:02, 56.44it/s]\u001b[A\n",
      " 57%|█████▋    | 153/270 [00:02<00:02, 56.35it/s]\u001b[A\n",
      " 59%|█████▉    | 159/270 [00:02<00:01, 56.27it/s]\u001b[A\n",
      " 61%|██████    | 165/270 [00:02<00:01, 56.15it/s]\u001b[A\n",
      " 63%|██████▎   | 171/270 [00:03<00:01, 56.14it/s]\u001b[A\n",
      " 66%|██████▌   | 177/270 [00:03<00:01, 55.99it/s]\u001b[A\n",
      " 68%|██████▊   | 183/270 [00:03<00:01, 55.46it/s]\u001b[A\n",
      " 70%|███████   | 189/270 [00:03<00:01, 54.97it/s]\u001b[A\n",
      " 72%|███████▏  | 195/270 [00:03<00:01, 54.93it/s]\u001b[A\n",
      " 74%|███████▍  | 201/270 [00:03<00:01, 54.86it/s]\u001b[A\n",
      " 77%|███████▋  | 207/270 [00:03<00:01, 54.25it/s]\u001b[A\n",
      " 79%|███████▊  | 212/270 [00:03<00:01, 54.13it/s]\u001b[A\n",
      " 82%|████████▏ | 221/270 [00:04<00:00, 54.88it/s]\u001b[A\n",
      " 86%|████████▌ | 231/270 [00:04<00:00, 55.75it/s]\u001b[A\n",
      " 88%|████████▊ | 238/270 [00:04<00:00, 55.73it/s]\u001b[A\n",
      " 91%|█████████ | 245/270 [00:04<00:00, 55.76it/s]\u001b[A\n",
      " 93%|█████████▎| 252/270 [00:04<00:00, 55.78it/s]\u001b[A\n",
      " 96%|█████████▌| 258/270 [00:04<00:00, 55.77it/s]\u001b[A\n",
      " 98%|█████████▊| 264/270 [00:04<00:00, 55.74it/s]\u001b[A\n",
      "100%|██████████| 270/270 [00:04<00:00, 55.76it/s]\u001b[A\n",
      "\u001b[A\n",
      "  0%|          | 0/270 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 6/270 [00:00<00:04, 59.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: nan, Test Loss : nan, Test Accuracy : 0.0, Train Accuracy : 0.0 : Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 12/270 [00:00<00:04, 56.57it/s]\u001b[A\n",
      "  7%|▋         | 18/270 [00:00<00:04, 54.95it/s]\u001b[A\n",
      "  9%|▉         | 24/270 [00:00<00:04, 55.15it/s]\u001b[A\n",
      " 11%|█         | 30/270 [00:00<00:04, 55.46it/s]\u001b[A\n",
      " 15%|█▍        | 40/270 [00:00<00:03, 61.92it/s]\u001b[A\n",
      " 18%|█▊        | 48/270 [00:00<00:03, 63.24it/s]\u001b[A\n",
      " 20%|██        | 55/270 [00:00<00:03, 61.87it/s]\u001b[A\n",
      " 23%|██▎       | 62/270 [00:01<00:03, 60.99it/s]\u001b[A\n",
      " 25%|██▌       | 68/270 [00:01<00:03, 60.29it/s]\u001b[A\n",
      " 27%|██▋       | 74/270 [00:01<00:03, 58.90it/s]\u001b[A\n",
      " 30%|██▉       | 80/270 [00:01<00:03, 57.06it/s]\u001b[A\n",
      " 32%|███▏      | 86/270 [00:01<00:03, 56.75it/s]\u001b[A\n",
      " 34%|███▍      | 92/270 [00:01<00:03, 56.56it/s]\u001b[A\n",
      " 36%|███▋      | 98/270 [00:01<00:03, 56.29it/s]\u001b[A\n",
      " 39%|███▊      | 104/270 [00:01<00:02, 56.02it/s]\u001b[A\n",
      " 41%|████      | 110/270 [00:02<00:02, 54.85it/s]\u001b[A\n",
      " 43%|████▎     | 115/270 [00:02<00:02, 54.31it/s]\u001b[A\n",
      " 45%|████▍     | 121/270 [00:02<00:02, 54.39it/s]\u001b[A\n",
      " 48%|████▊     | 129/270 [00:02<00:02, 55.29it/s]\u001b[A\n",
      " 51%|█████▏    | 139/270 [00:02<00:02, 56.96it/s]\u001b[A\n",
      " 54%|█████▍    | 146/270 [00:02<00:02, 56.93it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-20633b3aad98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#batch training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36msame_process_iter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0msame_process_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_sampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchify_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_in_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_pinned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gluonnlp/data/batchify/batchify.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mele_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gluonnlp/data/batchify/batchify.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mbatch_data\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \"\"\"\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_stack_arrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gluonnlp/data/batchify/batchify.py\u001b[0m in \u001b[0;36m_stack_arrs\u001b[0;34m(arrs, use_shared_mem, dtype)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_shared_mem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu_shared'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/utils.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sparse_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(source_array, ctx, dtype)\u001b[0m\n\u001b[1;32m   2433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source_array must be array like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2435\u001b[0;31m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2436\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_indexing_dispatch_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NDARRAY_BASIC_INDEXING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_nd_basic_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mindexing_dispatch_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NDARRAY_ADVANCED_INDEXING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_nd_advanced_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m_set_nd_basic_indexing\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    708\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_copyfrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# value might be a list or a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                     \u001b[0mvalue_nd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m_sync_copyfrom\u001b[0;34m(self, source_array)\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msource_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m             ctypes.c_size_t(source_array.size)))\n\u001b[0m\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "\n",
    "tot_test_loss = []\n",
    "tot_test_accu = []\n",
    "tot_train_loss = []\n",
    "tot_train_accu = []\n",
    "tot_valid_accu = [] \n",
    "for e in range(epochs):\n",
    "    #batch training \n",
    "    for i, (data, length, label) in enumerate(tqdm(train_dataloader)):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        length = length.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = model(data.T, length)\n",
    "            loss_ = loss(output, label)\n",
    "            loss_.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "    #caculate test loss\n",
    "    if e % 10 == 0: \n",
    "        test_loss = calculate_loss(model, test_dataloader, loss_obj = loss, ctx=ctx) \n",
    "        train_loss = calculate_loss(model, train_dataloader, loss_obj = loss, ctx=ctx) \n",
    "        test_accu = evaluate_accuracy(model, test_dataloader,  ctx=ctx)\n",
    "        train_accu = evaluate_accuracy(model, train_dataloader,  ctx=ctx)\n",
    "        valid_accu = evaluate_accuracy(model, valid_dataloader,  ctx=ctx)\n",
    "\n",
    "        print(\"Epoch %s. Train Loss: %s, Test Loss : %s,\" \\\n",
    "        \" Test Accuracy : %s,\" \\\n",
    "        \" Train Accuracy : %s : Valid Accuracy : %s\" % (e, train_loss, test_loss, test_accu, train_accu, valid_accu))    \n",
    "        tot_test_loss.append(test_loss)\n",
    "        tot_train_loss.append(train_loss)\n",
    "        tot_test_accu.append(test_accu)\n",
    "        tot_train_accu.append(train_accu)\n",
    "        tot_valid_accu.append(valid_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model export and Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Netron으로 네트워크 시각화 \n",
    "\n",
    "- https://lutzroeder.github.io/netron/\n",
    "- 저장된 `model-symbol.json`을 입력해 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = gluon.nn.SymbolBlock.imports(\"model-symbol.json\", ['data0', 'data1'], \"model-0000.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entitytag(sent):\n",
    "    sent_len = len(sent)\n",
    "    coded_sent = vocab_sent[length_clip(sent)]\n",
    "    co = nd.array(coded_sent).expand_dims(axis=1)\n",
    "    ret_code = load_model(co, nd.array([sent_len,]))\n",
    "    ret_seq = vocab_entity.to_tokens(ret_code.argmax(axis=2)[0].asnumpy().astype('int').tolist())\n",
    "    return(''.join(ret_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entitytag(\"모두의 연구소에 대해서 찾아줘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Test Accuracy 95% 이상 올리기\n",
    "- test_hidden 셋의 성능 90% 이상 올리기 \n",
    "- Entity Tagging과 Intent Classification을 MultiTask Learning으로 통합해보기(성능이 좋아지나? 나빠지나?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
